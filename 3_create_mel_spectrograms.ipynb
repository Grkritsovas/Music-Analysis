{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee4175b-1379-402e-a934-194764686a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d87758d-17c4-42c4-b941-3c4ce6b51307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(file_path):\n",
    "    # Load the CSV dataset\n",
    "    if file_path.endswith('.xlsx'):\n",
    "        df = pd.read_excel(file_path)\n",
    "    else:\n",
    "        df = pd.read_csv(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db881113-a744-4fc4-ae30-db5ed6aaf887",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_path = \"../datasets/Greek_Music_Dataset.xlsx\"\n",
    "df = load_df(excel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00fd10a-8380-465c-bc91-afc12785f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"../datasets/Dataset.csv\"\n",
    "df_csv = load_df(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3d6b4f-b707-4dce-8655-9568123ef5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_downloaded_mapping(downloaded_mapping_dir):\n",
    "    # Load the downloaded mapping\n",
    "    with open(downloaded_mapping_dir, 'r') as fp:\n",
    "        downloaded_mapping = json.load(fp)\n",
    "    return downloaded_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6084e948-418c-4c46-8a0e-c398b547f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_mapping_dir = '../downloaded_mapping_wav_updated.json'\n",
    "downloaded_mapping = load_downloaded_mapping(downloaded_mapping_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205ab5be-1310-4cc5-b198-8b677d4967ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4610dca1-7519-41d2-a481-6ab0de21e625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>YouTube Link</th>\n",
       "      <th>MIDI</th>\n",
       "      <th>VALENCE(A-D)</th>\n",
       "      <th>AROUSAL(1-4)</th>\n",
       "      <th>LAIKO</th>\n",
       "      <th>REMPETIKO</th>\n",
       "      <th>ENTEXNO</th>\n",
       "      <th>ROCK</th>\n",
       "      <th>Mod LAIKO</th>\n",
       "      <th>POP</th>\n",
       "      <th>ENALLAKTIKO</th>\n",
       "      <th>HIPHOP/RNB</th>\n",
       "      <th>LAST.FM IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Magic de spell</td>\n",
       "      <td>Pes Oti Thes</td>\n",
       "      <td>http://www.youtube.com/watch?v=Uu1jAlnijVI</td>\n",
       "      <td>No</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trypes</td>\n",
       "      <td>Xartino tsirko</td>\n",
       "      <td>http://www.youtube.com/watch?v=eXuxMWbYEro</td>\n",
       "      <td>No</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>332cf614-0232-463e-b460-3bfb1a8605a2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trypes</td>\n",
       "      <td>O,ti skotwneis einai diko sou gia panta</td>\n",
       "      <td>http://www.youtube.com/watch?v=Mzz3f451dF8</td>\n",
       "      <td>No</td>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>63904997-e498-44d1-be96-919fc5d10721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trypes</td>\n",
       "      <td>Patrida mou einai ekei pou mishsa</td>\n",
       "      <td>http://www.youtube.com/watch?v=A2DRKroYb6A</td>\n",
       "      <td>No</td>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1bd48c3f-5947-4444-a5db-3829e9ff1a1f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trypes</td>\n",
       "      <td>Tsakismenh xara</td>\n",
       "      <td>http://www.youtube.com/watch?v=EzbP4hcadg4</td>\n",
       "      <td>No</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>849fea0f-a15a-4cc8-8b7a-91ce9618a3ee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Artist                                     Song  \\\n",
       "0  Magic de spell                             Pes Oti Thes   \n",
       "1          Trypes                           Xartino tsirko   \n",
       "2          Trypes  O,ti skotwneis einai diko sou gia panta   \n",
       "3          Trypes        Patrida mou einai ekei pou mishsa   \n",
       "4          Trypes                          Tsakismenh xara   \n",
       "\n",
       "                                 YouTube Link MIDI VALENCE(A-D)  AROUSAL(1-4)  \\\n",
       "0  http://www.youtube.com/watch?v=Uu1jAlnijVI   No            A             3   \n",
       "1  http://www.youtube.com/watch?v=eXuxMWbYEro   No            A             3   \n",
       "2  http://www.youtube.com/watch?v=Mzz3f451dF8   No            D             4   \n",
       "3  http://www.youtube.com/watch?v=A2DRKroYb6A   No            B             2   \n",
       "4  http://www.youtube.com/watch?v=EzbP4hcadg4   No            A             3   \n",
       "\n",
       "  LAIKO REMPETIKO ENTEXNO ROCK Mod LAIKO POP ENALLAKTIKO HIPHOP/RNB  \\\n",
       "0    No        No      No  Yes        No  No          No         No   \n",
       "1    No        No      No  Yes        No  No         Yes         No   \n",
       "2    No        No      No  Yes        No  No         Yes         No   \n",
       "3    No        No      No  Yes        No  No         Yes         No   \n",
       "4    No        No      No  Yes        No  No          No         No   \n",
       "\n",
       "                            LAST.FM IDs  \n",
       "0                                   NaN  \n",
       "1  332cf614-0232-463e-b460-3bfb1a8605a2  \n",
       "2  63904997-e498-44d1-be96-919fc5d10721  \n",
       "3  1bd48c3f-5947-4444-a5db-3829e9ff1a1f  \n",
       "4  849fea0f-a15a-4cc8-8b7a-91ce9618a3ee  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "add256be-7ba2-464f-a72f-f76e7952f37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1084"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(downloaded_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cf4e5a7-ba1f-40db-adf8-1df9f238c3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARTIST'</th>\n",
       "      <th>'SONG'</th>\n",
       "      <th>'BEST_URL_FOUND'</th>\n",
       "      <th>'GENRE'</th>\n",
       "      <th>'VALENCE'</th>\n",
       "      <th>'AROUSAL'</th>\n",
       "      <th>'Spectral Centroid Overall Standard Deviation0'</th>\n",
       "      <th>'Spectral Rolloff Point Overall Standard Deviation0'</th>\n",
       "      <th>'Spectral Flux Overall Standard Deviation0'</th>\n",
       "      <th>'Compactness Overall Standard Deviation0'</th>\n",
       "      <th>...</th>\n",
       "      <th>'LPC Overall Standard Deviation8'</th>\n",
       "      <th>'LPC Overall Average0'</th>\n",
       "      <th>'LPC Overall Average1'</th>\n",
       "      <th>'LPC Overall Average2'</th>\n",
       "      <th>'LPC Overall Average3'</th>\n",
       "      <th>'LPC Overall Average4'</th>\n",
       "      <th>'LPC Overall Average5'</th>\n",
       "      <th>'LPC Overall Average6'</th>\n",
       "      <th>'LPC Overall Average7'</th>\n",
       "      <th>'LPC Overall Average8'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Magic de spell</td>\n",
       "      <td>Pes Oti Thes</td>\n",
       "      <td>http://www.youtube.com/watch?v=Uu1jAlnijVI</td>\n",
       "      <td>Rock</td>\n",
       "      <td>A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.70</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>252.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09462</td>\n",
       "      <td>-0.8072</td>\n",
       "      <td>0.19410</td>\n",
       "      <td>-0.220300</td>\n",
       "      <td>-0.110000</td>\n",
       "      <td>-0.14790</td>\n",
       "      <td>-0.078860</td>\n",
       "      <td>-0.025780</td>\n",
       "      <td>0.014930</td>\n",
       "      <td>-0.003004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trypes</td>\n",
       "      <td>Xartino tsirko</td>\n",
       "      <td>http://www.youtube.com/watch?v=eXuxMWbYEro</td>\n",
       "      <td>Rock</td>\n",
       "      <td>A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.68</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>209.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09725</td>\n",
       "      <td>-0.8688</td>\n",
       "      <td>0.27820</td>\n",
       "      <td>-0.106300</td>\n",
       "      <td>-0.048660</td>\n",
       "      <td>-0.04170</td>\n",
       "      <td>-0.015060</td>\n",
       "      <td>-0.051120</td>\n",
       "      <td>-0.001364</td>\n",
       "      <td>-0.018670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trypes,\"O,ti skotwneis einai diko sou gia pant...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trypes</td>\n",
       "      <td>Patrida mou einai ekei pou mishsa</td>\n",
       "      <td>http://www.youtube.com/watch?v=A2DRKroYb6A</td>\n",
       "      <td>Rock</td>\n",
       "      <td>B</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.07</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>160.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08025</td>\n",
       "      <td>-0.8607</td>\n",
       "      <td>0.23920</td>\n",
       "      <td>-0.095720</td>\n",
       "      <td>-0.073960</td>\n",
       "      <td>-0.08900</td>\n",
       "      <td>-0.050790</td>\n",
       "      <td>-0.035150</td>\n",
       "      <td>-0.023830</td>\n",
       "      <td>-0.029570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trypes</td>\n",
       "      <td>Tsakismenh xara</td>\n",
       "      <td>http://www.youtube.com/watch?v=EzbP4hcadg4</td>\n",
       "      <td>Rock</td>\n",
       "      <td>A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.87</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.011310</td>\n",
       "      <td>250.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09623</td>\n",
       "      <td>-0.8392</td>\n",
       "      <td>0.22380</td>\n",
       "      <td>-0.237400</td>\n",
       "      <td>0.021980</td>\n",
       "      <td>-0.07537</td>\n",
       "      <td>-0.013400</td>\n",
       "      <td>-0.050370</td>\n",
       "      <td>-0.002218</td>\n",
       "      <td>-0.014280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Hmiskoumpria</td>\n",
       "      <td>An hsoun allos</td>\n",
       "      <td>http://www.youtube.com/watch?v=iustoWWrNDA</td>\n",
       "      <td>Hip Hop/R &amp; B</td>\n",
       "      <td>B</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.57</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>393.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10940</td>\n",
       "      <td>-0.8380</td>\n",
       "      <td>0.31490</td>\n",
       "      <td>-0.089160</td>\n",
       "      <td>0.006647</td>\n",
       "      <td>0.01911</td>\n",
       "      <td>0.011590</td>\n",
       "      <td>-0.016750</td>\n",
       "      <td>-0.000251</td>\n",
       "      <td>0.006333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Hmiskoumpria</td>\n",
       "      <td>Apolyomai re Giwrgrh</td>\n",
       "      <td>http://www.youtube.com/watch?v=tzLY0FBXLaI</td>\n",
       "      <td>Hip Hop/R &amp; B</td>\n",
       "      <td>C</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.51</td>\n",
       "      <td>0.2041</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>337.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10500</td>\n",
       "      <td>-0.8426</td>\n",
       "      <td>0.22390</td>\n",
       "      <td>-0.065140</td>\n",
       "      <td>-0.025030</td>\n",
       "      <td>0.02171</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.006081</td>\n",
       "      <td>0.006013</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>BO</td>\n",
       "      <td>Tha thela</td>\n",
       "      <td>http://www.youtube.com/watch?v=JeF9mDSBk_4</td>\n",
       "      <td>Hip Hop/R &amp; B</td>\n",
       "      <td>C</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.46</td>\n",
       "      <td>0.1507</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>202.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10050</td>\n",
       "      <td>-0.8999</td>\n",
       "      <td>0.26850</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>-0.013890</td>\n",
       "      <td>-0.07782</td>\n",
       "      <td>-0.045750</td>\n",
       "      <td>-0.015370</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>-0.011950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>BO</td>\n",
       "      <td>Exe xarh</td>\n",
       "      <td>http://www.youtube.com/watch?v=P6WsQaH2teE</td>\n",
       "      <td>Hip Hop/R &amp; B</td>\n",
       "      <td>D</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.63</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>0.006737</td>\n",
       "      <td>230.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10110</td>\n",
       "      <td>-0.8961</td>\n",
       "      <td>0.06986</td>\n",
       "      <td>0.039190</td>\n",
       "      <td>0.040950</td>\n",
       "      <td>0.01792</td>\n",
       "      <td>0.041290</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.027590</td>\n",
       "      <td>-0.027180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>BO</td>\n",
       "      <td>Pio konta</td>\n",
       "      <td>http://www.youtube.com/watch?v=W-IepjxAPC4</td>\n",
       "      <td>Hip Hop/R &amp; B</td>\n",
       "      <td>C</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.34</td>\n",
       "      <td>0.1716</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>163.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09682</td>\n",
       "      <td>-0.8792</td>\n",
       "      <td>0.11650</td>\n",
       "      <td>0.024980</td>\n",
       "      <td>-0.026240</td>\n",
       "      <td>-0.03749</td>\n",
       "      <td>-0.005177</td>\n",
       "      <td>0.003069</td>\n",
       "      <td>-0.020910</td>\n",
       "      <td>-0.001413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 348 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               ARTIST'  \\\n",
       "0                                       Magic de spell   \n",
       "1                                               Trypes   \n",
       "2    Trypes,\"O,ti skotwneis einai diko sou gia pant...   \n",
       "3                                               Trypes   \n",
       "4                                               Trypes   \n",
       "..                                                 ...   \n",
       "995                                       Hmiskoumpria   \n",
       "996                                       Hmiskoumpria   \n",
       "997                                                 BO   \n",
       "998                                                 BO   \n",
       "999                                                 BO   \n",
       "\n",
       "                                'SONG'  \\\n",
       "0                         Pes Oti Thes   \n",
       "1                       Xartino tsirko   \n",
       "2                                  NaN   \n",
       "3    Patrida mou einai ekei pou mishsa   \n",
       "4                      Tsakismenh xara   \n",
       "..                                 ...   \n",
       "995                     An hsoun allos   \n",
       "996               Apolyomai re Giwrgrh   \n",
       "997                          Tha thela   \n",
       "998                           Exe xarh   \n",
       "999                          Pio konta   \n",
       "\n",
       "                               'BEST_URL_FOUND'        'GENRE' 'VALENCE'  \\\n",
       "0    http://www.youtube.com/watch?v=Uu1jAlnijVI           Rock         A   \n",
       "1    http://www.youtube.com/watch?v=eXuxMWbYEro           Rock         A   \n",
       "2                                           NaN            NaN       NaN   \n",
       "3    http://www.youtube.com/watch?v=A2DRKroYb6A           Rock         B   \n",
       "4    http://www.youtube.com/watch?v=EzbP4hcadg4           Rock         A   \n",
       "..                                          ...            ...       ...   \n",
       "995  http://www.youtube.com/watch?v=iustoWWrNDA  Hip Hop/R & B         B   \n",
       "996  http://www.youtube.com/watch?v=tzLY0FBXLaI  Hip Hop/R & B         C   \n",
       "997  http://www.youtube.com/watch?v=JeF9mDSBk_4  Hip Hop/R & B         C   \n",
       "998  http://www.youtube.com/watch?v=P6WsQaH2teE  Hip Hop/R & B         D   \n",
       "999  http://www.youtube.com/watch?v=W-IepjxAPC4  Hip Hop/R & B         C   \n",
       "\n",
       "     'AROUSAL'  'Spectral Centroid Overall Standard Deviation0'  \\\n",
       "0          3.0                                            19.70   \n",
       "1          3.0                                            17.68   \n",
       "2          NaN                                              NaN   \n",
       "3          2.0                                            16.07   \n",
       "4          3.0                                            17.87   \n",
       "..         ...                                              ...   \n",
       "995        2.0                                            26.57   \n",
       "996        2.0                                            30.51   \n",
       "997        4.0                                            19.46   \n",
       "998        2.0                                            16.63   \n",
       "999        2.0                                            22.34   \n",
       "\n",
       "     'Spectral Rolloff Point Overall Standard Deviation0'  \\\n",
       "0                                               0.1782      \n",
       "1                                               0.1418      \n",
       "2                                                  NaN      \n",
       "3                                               0.1490      \n",
       "4                                               0.1628      \n",
       "..                                                 ...      \n",
       "995                                             0.1739      \n",
       "996                                             0.2041      \n",
       "997                                             0.1507      \n",
       "998                                             0.1660      \n",
       "999                                             0.1716      \n",
       "\n",
       "     'Spectral Flux Overall Standard Deviation0'  \\\n",
       "0                                       0.001928   \n",
       "1                                       0.002184   \n",
       "2                                            NaN   \n",
       "3                                       0.001264   \n",
       "4                                       0.011310   \n",
       "..                                           ...   \n",
       "995                                     0.001541   \n",
       "996                                     0.008222   \n",
       "997                                     0.007843   \n",
       "998                                     0.006737   \n",
       "999                                     0.000119   \n",
       "\n",
       "     'Compactness Overall Standard Deviation0'  ...  \\\n",
       "0                                        252.8  ...   \n",
       "1                                        209.6  ...   \n",
       "2                                          NaN  ...   \n",
       "3                                        160.2  ...   \n",
       "4                                        250.9  ...   \n",
       "..                                         ...  ...   \n",
       "995                                      393.5  ...   \n",
       "996                                      337.7  ...   \n",
       "997                                      202.9  ...   \n",
       "998                                      230.6  ...   \n",
       "999                                      163.1  ...   \n",
       "\n",
       "     'LPC Overall Standard Deviation8'  'LPC Overall Average0'  \\\n",
       "0                              0.09462                 -0.8072   \n",
       "1                              0.09725                 -0.8688   \n",
       "2                                  NaN                     NaN   \n",
       "3                              0.08025                 -0.8607   \n",
       "4                              0.09623                 -0.8392   \n",
       "..                                 ...                     ...   \n",
       "995                            0.10940                 -0.8380   \n",
       "996                            0.10500                 -0.8426   \n",
       "997                            0.10050                 -0.8999   \n",
       "998                            0.10110                 -0.8961   \n",
       "999                            0.09682                 -0.8792   \n",
       "\n",
       "     'LPC Overall Average1'  'LPC Overall Average2'  'LPC Overall Average3'  \\\n",
       "0                   0.19410               -0.220300               -0.110000   \n",
       "1                   0.27820               -0.106300               -0.048660   \n",
       "2                       NaN                     NaN                     NaN   \n",
       "3                   0.23920               -0.095720               -0.073960   \n",
       "4                   0.22380               -0.237400                0.021980   \n",
       "..                      ...                     ...                     ...   \n",
       "995                 0.31490               -0.089160                0.006647   \n",
       "996                 0.22390               -0.065140               -0.025030   \n",
       "997                 0.26850               -0.000249               -0.013890   \n",
       "998                 0.06986                0.039190                0.040950   \n",
       "999                 0.11650                0.024980               -0.026240   \n",
       "\n",
       "     'LPC Overall Average4'  'LPC Overall Average5'  'LPC Overall Average6'  \\\n",
       "0                  -0.14790               -0.078860               -0.025780   \n",
       "1                  -0.04170               -0.015060               -0.051120   \n",
       "2                       NaN                     NaN                     NaN   \n",
       "3                  -0.08900               -0.050790               -0.035150   \n",
       "4                  -0.07537               -0.013400               -0.050370   \n",
       "..                      ...                     ...                     ...   \n",
       "995                 0.01911                0.011590               -0.016750   \n",
       "996                 0.02171                0.017700                0.006081   \n",
       "997                -0.07782               -0.045750               -0.015370   \n",
       "998                 0.01792                0.041290                0.029870   \n",
       "999                -0.03749               -0.005177                0.003069   \n",
       "\n",
       "     'LPC Overall Average7'  'LPC Overall Average8'  \n",
       "0                  0.014930               -0.003004  \n",
       "1                 -0.001364               -0.018670  \n",
       "2                       NaN                     NaN  \n",
       "3                 -0.023830               -0.029570  \n",
       "4                 -0.002218               -0.014280  \n",
       "..                      ...                     ...  \n",
       "995               -0.000251                0.006333  \n",
       "996                0.006013                0.022900  \n",
       "997                0.031780               -0.011950  \n",
       "998                0.027590               -0.027180  \n",
       "999               -0.020910               -0.001413  \n",
       "\n",
       "[1000 rows x 348 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb4be92-cdda-4e6c-9760-5deb274a6659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1086 songs in CSV that were downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Create a new column in the dataframe that indicates whether the song was downloaded.\n",
    "df['downloaded'] = df[\"YouTube Link\"].apply(lambda x: x in downloaded_mapping)\n",
    "\n",
    "# Filter to only the downloaded songs.\n",
    "df_downloaded = df[df['downloaded']].copy()\n",
    "\n",
    "print(f\"Found {len(df_downloaded)} songs in CSV that were downloaded.\")\n",
    "\n",
    "# New column with the local filename, can be used to ensure homogenous naming:\n",
    "df_downloaded['local_filename'] = df_downloaded[\"YouTube Link\"].map(downloaded_mapping)\n",
    "\n",
    "# Now, df_downloaded has the target values and the corresponding local filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66405a30-307a-43a0-8f27-94ff8e1c48a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1086"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a79bb6fc-171a-4bd9-b6d8-120fa96c983c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Yes\n",
       "1       Yes\n",
       "2       Yes\n",
       "3       Yes\n",
       "4       Yes\n",
       "       ... \n",
       "1395     Νο\n",
       "1396     Νο\n",
       "1397     Νο\n",
       "1398     Νο\n",
       "1399     Νο\n",
       "Name: ROCK, Length: 1086, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_downloaded[\"ROCK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "950f528f-bbe8-4bc6-9361-48d2910ce671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_silence(y, sr, top_db=25):\n",
    "    return librosa.effects.trim(y, top_db=top_db)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26d1c9cf-8454-4fc0-bc80-0c616a684986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_audio(y, sr, window_duration=30, hop_duration=15):\n",
    "    \"\"\"\n",
    "    Segments audio into overlapping segments.\n",
    "    \n",
    "    Parameters:\n",
    "      y: Audio time series.\n",
    "      sr: Sample rate.\n",
    "      window_duration: Duration (in seconds) of each segment.\n",
    "      hop_duration: Hop duration (in seconds) between segments.\n",
    "      \n",
    "    Returns:\n",
    "      List of audio segments.\n",
    "    \"\"\"\n",
    "    window_length = int(window_duration * sr)\n",
    "    hop_length_samples = int(hop_duration * sr)\n",
    "    segments = []\n",
    "    for start in range(0, len(y) - window_length + 1, hop_length_samples):\n",
    "        segment = y[start:start + window_length]\n",
    "        segments.append(segment)\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af44f683-133a-4b44-93a1-33ec441f4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mel_spectrogram(y, sr, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    \"\"\"Computes the mel spectrogram in decibel scale.\"\"\"\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    return librosa.power_to_db(mel_spec, ref=np.max).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9acdaae-98c8-4be6-8904-4d2025b372c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ab910da-6e9e-4cdb-9990-4c89a54fdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_batched(\n",
    "    df,\n",
    "    audio_folders,              # List of folders where the audio files reside.\n",
    "    sr=22050,\n",
    "    window_duration=30,         # Seconds per segment.\n",
    "    hop_duration=15,            # Seconds between segments.\n",
    "    num_splits=16,       # For high volumes of data increase number of splits to avoid RAM issues.\n",
    "    song_key_column=\"Song\",\n",
    "    genre_order=None,           # List of genre column names in a fixed order.\n",
    "    output_file_base=\"mel_spectrogram_batches/data_part\"  # Base name for batch JSON files.\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame subset (either training or testing) in batches.\n",
    "    \n",
    "    For each song:\n",
    "      - Checks for the corresponding audio file (appending '.wav' if needed).\n",
    "      - Loads the full audio, trims silence, segments it, and computes a mel spectrogram for each segment.\n",
    "      - Builds a target vector based on genre flags (using the provided genre_order).\n",
    "    \n",
    "    Each batch is saved to a JSON file, and all song dictionaries are collected into a list.\n",
    "    \n",
    "    Returns:\n",
    "      dataset: A list of song dictionaries:\n",
    "         { \"song\": <song name>,\n",
    "           \"segments\": [mel_spec_seg1, mel_spec_seg2, ...],\n",
    "           \"target\": [0, 1, 0, ...] }\n",
    "           \n",
    "      song_map: A dictionary mapping song names to their list of mel spectrogram segments.\n",
    "    \"\"\"\n",
    "    import os, json, librosa\n",
    "    import numpy as np\n",
    "    \n",
    "    # If a \"downloaded\" column exists, filter out songs not downloaded.\n",
    "    if \"downloaded\" in df.columns:\n",
    "        df = df[df[\"downloaded\"] == True]\n",
    "    \n",
    "    # Split the DataFrame into batches.\n",
    "    df_batches = np.array_split(df, num_splits)\n",
    "    dataset = []  # To collect processed song dictionaries.\n",
    "    song_map = {} # Mapping: song name -> list of mel spectrogram segments.\n",
    "    \n",
    "    for i, batch in enumerate(df_batches):\n",
    "        output_file = f\"{output_file_base}_{i+1}.json\"\n",
    "        print(f\"Processing batch {i+1}/{num_splits} - {len(batch)} songs...\")\n",
    "        with open(output_file, \"w\") as fp:\n",
    "            fp.write(\"{\\n\")\n",
    "        first_entry = True\n",
    "        \n",
    "        for idx, row in batch.iterrows():\n",
    "            song_name = row[song_key_column]\n",
    "            # Ensure the file name ends with '.wav'\n",
    "            candidate_file = song_name if song_name.lower().endswith(\".wav\") else song_name + \".wav\"\n",
    "            \n",
    "            # Search for the file in the provided audio folders.\n",
    "            file_found = False\n",
    "            file_path = None\n",
    "            for folder in audio_folders:\n",
    "                candidate_path = os.path.join(folder, candidate_file)\n",
    "                if os.path.exists(candidate_path):\n",
    "                    file_found = True\n",
    "                    file_path = candidate_path\n",
    "                    break\n",
    "            if not file_found:\n",
    "                print(f\"File not found for song '{song_name}' (tried '{candidate_file}')\")\n",
    "                continue  # Skip this song.\n",
    "            \n",
    "            # Build the target vector using the order in genre_order.\n",
    "            target_vector = []\n",
    "            if genre_order:\n",
    "                for genre in genre_order:\n",
    "                    val = row[genre]\n",
    "                    if isinstance(val, str):\n",
    "                        is_true = (val.strip().lower() in (\"yes\", \"true\", \"1\"))\n",
    "                    else:\n",
    "                        is_true = bool(val)\n",
    "                    target_vector.append(1 if is_true else 0)\n",
    "            \n",
    "            try:\n",
    "                # Load the full audio.\n",
    "                y, sr = librosa.load(file_path, sr=sr)\n",
    "                y = trim_silence(y, sr, top_db=30)\n",
    "                # Segment the audio.\n",
    "                segments = segment_audio(y, sr, window_duration, hop_duration)\n",
    "                # Compute mel spectrograms for each segment.\n",
    "                seg_specs = [compute_mel_spectrogram(seg, sr) for seg in segments]\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Build the song dictionary.\n",
    "            song_dict = {\n",
    "                \"song\": song_name,\n",
    "                \"segments\": seg_specs,\n",
    "                \"target\": target_vector\n",
    "            }\n",
    "            \n",
    "            # Write this entry to the batch JSON file.\n",
    "            entry_str = f'\"{song_name}\": {json.dumps(song_dict)}'\n",
    "            with open(output_file, \"a\") as fp:\n",
    "                if not first_entry:\n",
    "                    fp.write(\",\\n\")\n",
    "                else:\n",
    "                    first_entry = False\n",
    "                fp.write(entry_str)\n",
    "            \n",
    "            dataset.append(song_dict)\n",
    "            # Also add the mapping of song to its segments.\n",
    "            song_map[song_name] = seg_specs\n",
    "            print(f\"Processed '{song_name}' with {len(seg_specs)} segments.\")\n",
    "        \n",
    "        with open(output_file, \"a\") as fp:\n",
    "            fp.write(\"\\n}\")\n",
    "        print(f\"Batch {i+1} completed! Saved in {output_file}.\")\n",
    "\n",
    "        del batch\n",
    "        gc.collect()\n",
    "    \n",
    "    return dataset, song_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eb209b7-3bd5-4017-be29-13d2dca7c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_split_dataset(\n",
    "    df,\n",
    "    audio_folders,\n",
    "    sr=22050,\n",
    "    window_duration=30,\n",
    "    hop_duration=15,\n",
    "    train_ratio=0.8,        # 80% of songs for training\n",
    "    train_batches=6,\n",
    "    test_batches=2,\n",
    "    song_key_column=\"Song\",\n",
    "    genre_order=None,\n",
    "    train_output_base=\"mel_spectrogram_batches/train_data_part\",\n",
    "    test_output_base=\"mel_spectrogram_batches/test_data_part\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits the full DataFrame into training and testing sets at the song level,\n",
    "    then processes each subset in batches (segmenting each song, computing spectrograms,\n",
    "    and building the target vectors). Each batch is saved to its own JSON file.\n",
    "    \n",
    "    Returns:\n",
    "      train_data: list of song dictionaries from the training set.\n",
    "      test_data: list of song dictionaries from the testing set.\n",
    "      train_song_map: dictionary mapping song names to their spectrogram segments for training.\n",
    "      test_song_map: dictionary mapping song names to their spectrogram segments for testing.\n",
    "    \"\"\"\n",
    "    # Shuffle the DataFrame to randomize song order.\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    train_size = int(len(df) * train_ratio)\n",
    "    train_df = df.iloc[:train_size]\n",
    "    test_df = df.iloc[train_size:]\n",
    "    \n",
    "    print(f\"Total songs: {len(df)}. Training: {len(train_df)}. Testing: {len(test_df)}.\")\n",
    "    \n",
    "    print(\"Processing training data...\")\n",
    "    # Now process_dataset_batched is expected to return both dataset and song_map.\n",
    "    train_data, train_song_map = process_dataset_batched(\n",
    "        df=train_df,\n",
    "        audio_folders=audio_folders,\n",
    "        sr=sr,\n",
    "        window_duration=window_duration,\n",
    "        hop_duration=hop_duration,\n",
    "        num_splits=train_batches,\n",
    "        song_key_column=song_key_column,\n",
    "        genre_order=genre_order,\n",
    "        output_file_base=train_output_base\n",
    "    )\n",
    "    \n",
    "    print(\"Processing testing data...\")\n",
    "    test_data, test_song_map = process_dataset_batched(\n",
    "        df=test_df,\n",
    "        audio_folders=audio_folders,\n",
    "        sr=sr,\n",
    "        window_duration=window_duration,\n",
    "        hop_duration=hop_duration,\n",
    "        num_splits=test_batches,\n",
    "        song_key_column=song_key_column,\n",
    "        genre_order=genre_order,\n",
    "        output_file_base=test_output_base\n",
    "    )\n",
    "    \n",
    "    return train_data, test_data, train_song_map, test_song_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8950f73-269e-47e9-959f-7a5cddde3132",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folders = [\"../downloads_wav_1\", \"../downloads_wav_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bca1be09-d03d-4ae5-b9c6-9d6285a6d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_order=[\"LAIKO\", \"REMPETIKO\", \"ENTEXNO\", \"ROCK\", \"Mod LAIKO\", \"POP\", \"ENALLAKTIKO\", \"HIPHOP/RNB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf4bc2-99f8-45b1-afef-453ecb220200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total songs: 1086. Training: 868. Testing: 218.\n",
      "Processing training data...\n",
      "Processing batch 1/16 - 55 songs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/georgios/miniconda3/envs/tf-gpu/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 'Tsigara mpoukalia' with 11 segments.\n",
      "Processed 'To ax pou me lytrwnei' with 13 segments.\n",
      "Processed 'Ola kala' with 11 segments.\n",
      "Processed 'Stou profhth Hlia' with 12 segments.\n",
      "Processed 'Zaxaroponth' with 11 segments.\n",
      "Processed 'O kosmos phge paso' with 9 segments.\n",
      "Processed 'Kollhga gios' with 11 segments.\n",
      "Processed 'Dwdeka mantolina' with 7 segments.\n",
      "Processed 'To xeirokrothma' with 17 segments.\n",
      "Processed 'Vradies me aktinovolia' with 10 segments.\n",
      "Processed 'Dwse mou mia agkalia' with 10 segments.\n",
      "Processed 'Pes tou na paei na na na' with 13 segments.\n",
      "Processed 'Egw tragoudaga' with 15 segments.\n",
      "Processed 'Kserw kapoio asteri' with 9 segments.\n",
      "Processed 'Zeimpekiko' with 10 segments.\n",
      "Processed 'Pws thes na to kserw' with 9 segments.\n",
      "Processed 'Pali tha klapsw' with 11 segments.\n",
      "Processed 'Gh kai ouranos' with 15 segments.\n",
      "Processed 'Zhta mou oti thes' with 12 segments.\n",
      "Processed 'Daneika' with 17 segments.\n",
      "Processed 'Zhmeia' with 22 segments.\n",
      "Processed 'Den klaiw gia twra' with 9 segments.\n",
      "Processed 'Den eisai tipota' with 8 segments.\n",
      "Processed 'Ftaiei o erwtas' with 13 segments.\n",
      "Processed 'Apogeuma thlhmeno' with 11 segments.\n",
      "Processed 'Egw tha kopsw to krasi' with 9 segments.\n",
      "Processed 'O Loukas' with 12 segments.\n",
      "Processed 'To kalokairi efyge' with 16 segments.\n",
      "Processed 'To ogdoo thauma' with 13 segments.\n",
      "Processed 'Vasiliades mias strigmhs' with 25 segments.\n",
      "Processed 'Paidi mou wra sou kalh' with 10 segments.\n",
      "Processed 'Kale den me lypasai' with 10 segments.\n",
      "Processed 'Augoustos' with 14 segments.\n",
      "Processed 'Laos kai Kolwnaki' with 13 segments.\n",
      "Processed 'Egklhma ekanes' with 16 segments.\n",
      "Processed 'Sto Leuko Pyrgo' with 12 segments.\n",
      "Processed 'To agalma' with 12 segments.\n",
      "Processed 'Pythagoreion' with 21 segments.\n",
      "Processed 'Afou to thes' with 12 segments.\n",
      "Processed 'Edw yparxei agaph' with 13 segments.\n",
      "Processed 'Ena to xelidoni' with 12 segments.\n",
      "Processed 'Pw pw ti pathame' with 10 segments.\n",
      "Processed 'Zhse th stigmh' with 12 segments.\n",
      "Processed 'Pou eisai' with 12 segments.\n",
      "Processed 'E oxi ki etsi' with 14 segments.\n",
      "Processed 'Nanourisma' with 13 segments.\n",
      "Processed 'Alkoolika stixakia' with 12 segments.\n",
      "Processed 'Varia valitsa' with 16 segments.\n",
      "Processed 'Autapates se thola nera' with 13 segments.\n",
      "Processed 'Egw kai kamia' with 9 segments.\n",
      "Processed 'Egw ta lathi mou' with 14 segments.\n",
      "Processed 'Egina monos mou giatros' with 9 segments.\n",
      "Processed 'Ths agaphs aimata' with 14 segments.\n",
      "Processed 'O Ellhnas pou exeis synithisei' with 16 segments.\n",
      "Processed 'Sthn Athina' with 19 segments.\n",
      "Batch 1 completed! Saved in train_data_part_1.json.\n",
      "Processing batch 2/16 - 55 songs...\n",
      "Processed 'Sthn trela me ftaneis' with 17 segments.\n",
      "Processed 'Paikse paliatso ta tragoudia sou teleionoun' with 19 segments.\n",
      "Processed 'Grifos' with 9 segments.\n",
      "Processed 'O Isoviths' with 11 segments.\n",
      "Processed 'Allothi' with 10 segments.\n",
      "Processed 'H agaph den teleiwnei etsi apla' with 15 segments.\n",
      "Processed 'Dws mou ligaki shmasia' with 16 segments.\n",
      "Processed 'Agria aloga' with 14 segments.\n",
      "Processed 'Ase me sthn trela mou' with 13 segments.\n",
      "Processed 'Ti theleis na kanw' with 7 segments.\n",
      "Processed 'Egw tha zw me to oneiro' with 12 segments.\n",
      "Processed 'Zhtas viens viens' with 12 segments.\n",
      "Processed 'Vre pws allazoun oi kairoi' with 7 segments.\n",
      "Processed 'To myalo mou gyrnaei' with 13 segments.\n",
      "Processed 'Kyra - Giwrgaina' with 9 segments.\n",
      "Processed 'Ola afta pou den tha dw' with 18 segments.\n",
      "Processed 'Gia mena tragoudw' with 23 segments.\n",
      "Processed 'Menw se kapoia geitonia' with 12 segments.\n",
      "Processed 'Osh glyka exoun ta xeilh sou' with 10 segments.\n",
      "Processed 'Ena maxairi' with 10 segments.\n",
      "Processed 'Matia vourkwmena' with 10 segments.\n",
      "Processed 'Egw edw ki esy allou' with 11 segments.\n",
      "Processed 'Ti aisthanesai gia mena' with 17 segments.\n",
      "Processed 'Gelage h Maria' with 7 segments.\n",
      "Processed 'Pou tha paei pou tha bgei' with 7 segments.\n",
      "Processed 'Peta ta vivlia' with 10 segments.\n",
      "File not found for song 'Sygnwmh pou s'agaphsa poly' (tried 'Sygnwmh pou s'agaphsa poly.wav')\n",
      "Processed 'Bhmata' with 14 segments.\n",
      "Processed 'Zalh' with 13 segments.\n",
      "Processed 'Apousia' with 15 segments.\n",
      "Processed 'Ta dakrya mou einai kauta' with 11 segments.\n",
      "Processed 'Vrexei o Theos' with 11 segments.\n",
      "Processed 'Zhleia zhleia' with 14 segments.\n",
      "Processed 'Gia Thessalonikh Athina' with 18 segments.\n",
      "Processed 'To zeimpekiko  ths Athinas' with 15 segments.\n",
      "Processed 'Xairetismata' with 17 segments.\n",
      "Processed 'Aspro fws' with 18 segments.\n",
      "Processed 'Gia ta matia pou agapw' with 10 segments.\n",
      "Processed 'Egw gia dyo' with 13 segments.\n",
      "File not found for song 'Ap' t'aeroplano' (tried 'Ap' t'aeroplano.wav')\n",
      "Processed 'Aspra kokkina' with 9 segments.\n",
      "Processed 'Sthn K' with 10 segments.\n",
      "Processed 'Milhse mou' with 12 segments.\n",
      "Processed 'Kioumprik' with 15 segments.\n",
      "Processed 'Gynaika' with 21 segments.\n",
      "Processed 'Agapaw kai adiaforw' with 11 segments.\n",
      "Processed 'Na xa th dynamh' with 10 segments.\n",
      "Processed 'Siga siga' with 10 segments.\n",
      "Processed 'H modistroula' with 10 segments.\n",
      "Processed 'Egw ki o iskios mou' with 12 segments.\n",
      "Processed 'Xwris ntroph' with 14 segments.\n",
      "Processed 'Fysa me pio dynata' with 12 segments.\n",
      "Processed 'Zw th stigmh' with 13 segments.\n",
      "Processed 'Adiaforw' with 14 segments.\n",
      "Processed 'Me ti kardia na arnithw' with 12 segments.\n",
      "Batch 2 completed! Saved in train_data_part_2.json.\n",
      "Processing batch 3/16 - 55 songs...\n",
      "Processed 'Egw me thn agaph malwsa' with 15 segments.\n",
      "Processed 'An eisai magkas' with 12 segments.\n",
      "Processed 'Erwtiko' with 12 segments.\n",
      "Processed 'E nai loipon' with 13 segments.\n",
      "Processed 'Den yparxw' with 14 segments.\n",
      "Processed 'Senior' with 18 segments.\n",
      "Processed 'Akatallhlh skhnh' with 13 segments.\n",
      "Processed 'Amore mio' with 12 segments.\n",
      "Processed 'Sfaira' with 15 segments.\n",
      "Processed 'Agrimia ki agrimakia mou' with 12 segments.\n",
      "Processed 'Siwph' with 14 segments.\n",
      "Processed 'Zhse opws eisai' with 10 segments.\n",
      "Processed 'Podhlata dixws frena' with 18 segments.\n",
      "Processed 'Asta ta mallakia sou' with 14 segments.\n",
      "Processed 'Thessaloniki' with 11 segments.\n",
      "Processed 'Vrekse Thee mou' with 11 segments.\n",
      "Processed 'Axaristh' with 11 segments.\n",
      "Processed 'Zaxarh kai meli' with 15 segments.\n",
      "Processed 'Egw magkas fainomouna' with 10 segments.\n",
      "Processed 'Auto ton kosmo ton kalo' with 14 segments.\n",
      "Processed 'Eggys anatolh' with 13 segments.\n",
      "Processed 'Egw eimai h lena' with 11 segments.\n",
      "Processed 'Kokkino garyfallo' with 16 segments.\n",
      "Processed 'Gia ta adelfia pou xathikan nwris' with 16 segments.\n",
      "Processed 'Zhleuw' with 11 segments.\n",
      "Processed 'Zestos aeras' with 10 segments.\n",
      "Processed 'O dolofonos' with 13 segments.\n",
      "Processed 'Egw pote den tragoudw' with 16 segments.\n",
      "Processed 'Zhse monaxa to shmera' with 11 segments.\n",
      "Processed 'O Ilissos' with 12 segments.\n",
      "Processed 'Egw thelw' with 16 segments.\n",
      "Processed 'Ksexnas' with 14 segments.\n",
      "Processed 'Tha me thymithies' with 14 segments.\n",
      "Processed 'Aoratoi stauroi' with 10 segments.\n",
      "Processed 'Agios Frevrouarios' with 14 segments.\n",
      "Processed 'Egw gennhthika aetos' with 14 segments.\n",
      "Processed 'To gramma' with 10 segments.\n",
      "Processed 'Magkas vghke gia sergiani' with 12 segments.\n",
      "Processed 'To parapono' with 15 segments.\n",
      "Processed 'To thyma o Nikolos' with 11 segments.\n",
      "Processed 'Thelo na se dw' with 18 segments.\n",
      "Processed 'Anavw mia fwtia' with 11 segments.\n",
      "Processed 'Poula me' with 15 segments.\n",
      "Processed 'Ti einai auto pou mas xwrizei' with 12 segments.\n",
      "Processed 'Irtha kai apopse sta skalopatia sou' with 10 segments.\n",
      "Processed 'Egklhma kai timwria' with 7 segments.\n",
      "Processed 'Ta mple papoutsia' with 9 segments.\n",
      "Processed 'Exases to treno' with 10 segments.\n",
      "Processed 'Aretousa' with 16 segments.\n",
      "Processed 'Kshmerwmata' with 8 segments.\n",
      "Processed 'Ypomonh' with 9 segments.\n",
      "Processed 'Akth' with 14 segments.\n",
      "Processed 'Aneva sto trapezi mou' with 13 segments.\n",
      "Processed 'Patrida mou einai ekei pou mishsa' with 16 segments.\n",
      "Processed 'Egw eimai entaksei' with 11 segments.\n",
      "Batch 3 completed! Saved in train_data_part_3.json.\n",
      "Processing batch 4/16 - 55 songs...\n",
      "Processed 'Marigw h augoulou' with 12 segments.\n",
      "Processed 'To pelago einai vathy' with 7 segments.\n",
      "Processed 'Poia nomizei pws einai' with 16 segments.\n",
      "Processed 'H fwtografia' with 8 segments.\n",
      "File not found for song 'Vareli, piasthka skoini kordoni' (tried 'Vareli, piasthka skoini kordoni.wav')\n",
      "Processed 'Rempetes' with 12 segments.\n",
      "Processed 'Periergo paixnidi' with 14 segments.\n",
      "Processed 'Tipota' with 14 segments.\n",
      "Processed 'Ax Ellada' with 14 segments.\n",
      "Processed 'Koita me' with 13 segments.\n",
      "Processed 'Sto grafeio' with 12 segments.\n",
      "Processed 'Ola odhgoun se sena' with 16 segments.\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_song_map, test_song_map = process_and_split_dataset(\n",
    "    df=df_downloaded,                # your full DataFrame\n",
    "    audio_folders=audio_folders,\n",
    "    sr=22050,\n",
    "    window_duration=30,\n",
    "    hop_duration=15,\n",
    "    train_ratio=0.8,\n",
    "    train_batches=16,\n",
    "    test_batches=5,\n",
    "    song_key_column=\"Song\",\n",
    "    genre_order=genre_order,\n",
    "    train_output_base=\"train_data_part\",\n",
    "    test_output_base=\"test_data_part\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a5ced7f-06c6-4067-a291-296b71479f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "song_map saved to 'song_map.json'.\n"
     ]
    }
   ],
   "source": [
    "# Save the song mapping (song_map) to a JSON file.\n",
    "with open('song_map.json', 'w') as f:\n",
    "    json.dump(song_map, f)\n",
    "print(\"song_map saved to 'song_map.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e44d5fda-92fd-4c27-bd31-1067e77c75fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data saved to 'training_data.json'.\n"
     ]
    }
   ],
   "source": [
    "# Save the training data (list of [spectrogram, target_vector] pairs) to a JSON file.\n",
    "with open('training_data.json', 'w') as f:\n",
    "    json.dump(training_data, f)\n",
    "print(\"training_data saved to 'training_data.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51b20122-b7fc-476f-bf1f-3b9ea17591b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre_order saved to 'genre_order.json'.\n"
     ]
    }
   ],
   "source": [
    "# Save the genre order list so we know which index corresponds to which genre.\n",
    "genre_order = [\"LAIKO\", \"REMPETIKO\", \"ENTEXNO\", \"ROCK\", \"Mod LAIKO\", \"POP\", \"ENALLAKTIKO\", \"HIPHOP/RNB\"]\n",
    "with open('genre_order.json', 'w') as f:\n",
    "    json.dump(genre_order, f)\n",
    "print(\"genre_order saved to 'genre_order.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c6879e3-9c1d-4da5-8e44-3fdf3025b665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReferrence on how to use this later for training and evaluation. First load them.\\nimport json\\n\\n# Save the song mapping (song_map) to a JSON file.\\nwith open(\\'song_map.json\\', \\'w\\') as f:\\n    json.dump(song_map, f)\\nprint(\"song_map saved to \\'song_map.json\\'.\")\\n\\n# Save the training data (list of [spectrogram, target_vector] pairs) to a JSON file.\\nwith open(\\'training_data.json\\', \\'w\\') as f:\\n    json.dump(training_data, f)\\nprint(\"training_data saved to \\'training_data.json\\'.\")\\n\\n# Optionally, save the genre order list so you know which index corresponds to which genre.\\ngenre_order = [\"LAIKO\", \"REMPETIKO\", \"ENTEXNO\", \"ROCK\", \"Mod LAIKO\", \"POP\", \"ENALLAKTIKO\", \"HIPHOP/RNB\"]\\nwith open(\\'genre_order.json\\', \\'w\\') as f:\\n    json.dump(genre_order, f)\\nprint(\"genre_order saved to \\'genre_order.json\\'.\")\\n\\nThen:\\nimport pandas as pd\\n\\n# Create a DataFrame from the training data.\\ndf_train = pd.DataFrame(training_data, columns=[\"spectrogram\", \"target\"])\\n# Now, df_train[\"spectrogram\"] holds your features and df_train[\"target\"] your labels.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Referrence on how to use this later for training and evaluation. First load them.\n",
    "import json\n",
    "\n",
    "# Save the song mapping (song_map) to a JSON file.\n",
    "with open('song_map.json', 'w') as f:\n",
    "    json.dump(song_map, f)\n",
    "print(\"song_map saved to 'song_map.json'.\")\n",
    "\n",
    "# Save the training data (list of [spectrogram, target_vector] pairs) to a JSON file.\n",
    "with open('training_data.json', 'w') as f:\n",
    "    json.dump(training_data, f)\n",
    "print(\"training_data saved to 'training_data.json'.\")\n",
    "\n",
    "# Optionally, save the genre order list so you know which index corresponds to which genre.\n",
    "genre_order = [\"LAIKO\", \"REMPETIKO\", \"ENTEXNO\", \"ROCK\", \"Mod LAIKO\", \"POP\", \"ENALLAKTIKO\", \"HIPHOP/RNB\"]\n",
    "with open('genre_order.json', 'w') as f:\n",
    "    json.dump(genre_order, f)\n",
    "print(\"genre_order saved to 'genre_order.json'.\")\n",
    "\n",
    "Then:\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the training data.\n",
    "df_train = pd.DataFrame(training_data, columns=[\"spectrogram\", \"target\"])\n",
    "# Now, df_train[\"spectrogram\"] holds your features and df_train[\"target\"] your labels.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f11fd-98fb-410a-8446-aa301e0476b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the training data from JSON\n",
    "with open(\"train_data.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame for easy manipulation.\n",
    "df_train = pd.DataFrame(train_data)\n",
    "print(\"Data loaded. Sample:\")\n",
    "print(df_train.head())\n",
    "\n",
    "# For training, you only need segments and targets.\n",
    "# If you have multiple segments per song and want to train on each segment individually:\n",
    "X_train = []\n",
    "y_train = []\n",
    "for idx, row in df_train.iterrows():\n",
    "    # Each row's 'segments' is a list of segments for that song.\n",
    "    # Extend X_train and y_train accordingly.\n",
    "    for seg in row['segments']:\n",
    "        X_train.append(seg)\n",
    "        y_train.append(row['target'])\n",
    "\n",
    "# Alternatively, if you prefer keeping them grouped by song,\n",
    "# you can keep the segments as a list and later decide how to feed them to your CNN.\n",
    "# For training, you won't need the \"song\" column.\n",
    "df_train = df_train.drop(columns=[\"song\"])\n",
    "print(\"Training DataFrame prepared:\")\n",
    "print(df_train.head())\n",
    "\n",
    "# Now, X_train and y_train can be used to train your model.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
